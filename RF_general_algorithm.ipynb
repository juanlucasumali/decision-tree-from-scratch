{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: [[0, 3, 0], [1, 3, 1], [2, 1, 2], [0, 3, 0], [1, 3, 1], [2, 1, 2]]\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 0: [[0, 3, 0], [0, 3, 0]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 0: [[1, 3, 1], [2, 1, 2], [1, 3, 1], [2, 1, 2]]\n",
      "All classes in the dataset: [0, 1, 2, 0, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [0, 1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 1: [[1, 3, 1], [1, 3, 1]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 1: [[0, 3, 0], [2, 1, 2], [0, 3, 0], [2, 1, 2]]\n",
      "All classes in the dataset: [0, 1, 2, 0, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [0, 1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 2: [[2, 1, 2], [2, 1, 2]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 2: [[0, 3, 0], [1, 3, 1], [0, 3, 0], [1, 3, 1]]\n",
      "All classes in the dataset: [0, 1, 2, 0, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [0, 1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 1 and a feature instance of 3: [[0, 3, 0], [1, 3, 1], [0, 3, 0], [1, 3, 1]]\n",
      "Node 2 with a feature index of 1 and a feature instance of 3: [[2, 1, 2], [2, 1, 2]]\n",
      "All classes in the dataset: [0, 1, 2, 0, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [0, 1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 1 and a feature instance of 1: [[2, 1, 2], [2, 1, 2]]\n",
      "Node 2 with a feature index of 1 and a feature instance of 1: [[0, 3, 0], [1, 3, 1], [0, 3, 0], [1, 3, 1]]\n",
      "All classes in the dataset: [0, 1, 2, 0, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [0, 1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "All possible outcomes of the split: [[36.29041406655651, 0, 0], [29.0, 1, 1], [29.0, 0, 2], [29.0, 0, 1], [13.0, 1, 3]]\n",
      "Optimal feature index: 0\n",
      "Optimal feature instance: 0\n"
     ]
    }
   ],
   "source": [
    "# STEP 1 â€” RANDOM FOREST ALGORITHM:\n",
    "# a. Split data such that information gain (decrease in entropy after splitting) is high\n",
    "# b. Split the data using each condition, check the gain returned\n",
    "# c. Condition with highest gain will be used to make the first split\n",
    "# d. Keep splitting nodes UNTIL entropy is 0\n",
    "# e. Classify unknown data point with multiple trees\n",
    "\n",
    "practice_training_dataset_1 = [[0, 3, 0], \n",
    "                             [1, 3, 1], \n",
    "                             [2, 1, 2],  \n",
    "                             [0, 3, 0], \n",
    "                             [1, 3, 1], \n",
    "                             [2, 1, 2]]\n",
    "\n",
    "practice_training_dataset_2 = [[0, 3, 0], \n",
    "                               [1, 3, 1], \n",
    "                               [2, 1, 2],  \n",
    "                               [0, 3, 1], \n",
    "                               [1, 3, 1], \n",
    "                               [2, 1, 2]]\n",
    "\n",
    "practice_unclassified_instance = [2, 1]\n",
    "\n",
    "# Entropy: Count of different classes in a list\n",
    "# Information gain: Difference between count of different classes in the current node \n",
    "#                   and the count of different classes in the previous node\n",
    "\n",
    "# Process:\n",
    "# 1. Get the entropy value of the root node\n",
    "# 2. Iterature through the database and split it into two arrays based on a condition\n",
    "# 3. Get the entropy value of the following nodes, find the difference between the \n",
    "#    root node entropy and the subsequent nodes\n",
    "# 4. Continue to split nodes with nonzero entropy until only leaf nodes remain\n",
    "\n",
    "import random\n",
    "\n",
    "# Creates a decision tree\n",
    "class DecisionTree:\n",
    "    \n",
    "    # Initializes the attributes of the DecisionTree class.\n",
    "    def __init__(self, dataset, number_of_features):\n",
    "        self.dataset = dataset\n",
    "        self.number_of_features = number_of_features\n",
    "        self.random_seed = 0\n",
    "\n",
    "        self.instances = []\n",
    "        self.feature_indicies = []\n",
    "        self.feature_instances = []\n",
    "        self.optimal_feature_index = 0\n",
    "        self.optimal_feature_instance = 0\n",
    "\n",
    "        self.feature = 0\n",
    "        self.feature_to_split = 0\n",
    "\n",
    "        self.possible_outcomes = []\n",
    "        \n",
    "        self.nodes = []\n",
    "        self.current_node = []\n",
    "\n",
    "    # Populates self.feature_instances with arrays of unique instances of each feature in the database\n",
    "    def get_features(self):\n",
    "        feature = 0\n",
    "        while feature < self.number_of_features:\n",
    "            for instance in self.dataset:\n",
    "                if instance[feature] not in self.instances:\n",
    "                    self.instances.append(instance[feature])\n",
    "            pair = [feature, self.instances]\n",
    "            self.feature_instances.append(pair)\n",
    "            feature += 1\n",
    "            self.instances = []\n",
    "\n",
    "        for feature_number in range(self.number_of_features):\n",
    "            self.feature_indicies.append(feature_number)\n",
    "\n",
    "        # self.random_seed = random.randrange(0, 101)\n",
    "        # random.seed(self.random_seed)\n",
    "\n",
    "        # self.feature = self.feature_indicies.pop(random.choice(self.feature_indicies))\n",
    "        # self.feature_to_split = random.choice(self.feature_instances[self.feature][1])\n",
    "\n",
    "        # 1. Get argument: how many features there are, random seed\n",
    "        # 2. Based on the number of features, use an algo to identify the different instances of that feature\n",
    "        # 3. Every time new node is called, randomize the feature and how it is split. Once used, remove from lists.\n",
    "\n",
    "    # Calculates the entropy of the current node\n",
    "    \n",
    "    def calculate_entropy(self, current_node):\n",
    "    \n",
    "        entropy = 0\n",
    "\n",
    "        # Create an array of the labels \n",
    "        labels = []\n",
    "        for instance in current_node:\n",
    "            labels.append(instance[-1])\n",
    "        print(f\"All classes in the dataset: {labels}\")\n",
    "        \n",
    "        unique_labels = []\n",
    "        for instance in current_node:\n",
    "            if instance[-1] not in unique_labels:\n",
    "                unique_labels.append(instance[-1])\n",
    "        print(f\"\\nAll unique labels in the dataset: {unique_labels}\")\n",
    "\n",
    "        # 1. Get the count of one unique label in labels and pop it all out (for loop with length of unique labels array)\n",
    "        # 2. Apply to the entropy formula \n",
    "        # 3. Repeat\n",
    "        \n",
    "        np_labels = np.array(labels)\n",
    "        for label in unique_labels:\n",
    "            # Using numpy to count the occurences of each label within the node\n",
    "            count = np.count_nonzero(np_labels == label)\n",
    "            entropy += -count / len(labels) * np.log(count / len(labels)) / np.log(len(unique_labels))\n",
    "        \n",
    "\n",
    "        print(f\"\\nCalculated entropy: {entropy}\")\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def calculate_weighted_average_times_child(self, child_node_1, child_node_2):\n",
    "        \n",
    "        entropy = 0\n",
    "        weighted_average_times_children = 0\n",
    "\n",
    "        for child_node in [child_node_1, child_node_2]:\n",
    "            unique_labels = []\n",
    "            for label in child_node:\n",
    "                if label not in unique_labels:\n",
    "                    unique_labels.append(label[-1])\n",
    "            \n",
    "            np_labels = np.array(child_node)\n",
    "            for label in unique_labels:\n",
    "                count = np.count_nonzero(np_labels == label)\n",
    "                entropy += -count / len(child_node) * np.log(count / len(child_node)) / np.log(len(unique_labels))\n",
    "                weighted_average_times_children += count / len(child_node) * entropy\n",
    "        \n",
    "        return weighted_average_times_children\n",
    "    \n",
    "    def calculate_information_gain(self, current_node, child_node_1, child_node_2):\n",
    "        watc = self.calculate_weighted_average_times_child(child_node_1, child_node_2)\n",
    "        IG = self.calculate_entropy(current_node) - watc\n",
    "        return IG\n",
    "        \n",
    "    # Adds the current node to a list of all nodes as a key-value pair with entropy as its key\n",
    "    def create_node(self, current_node, entropy):\n",
    "        pair = [entropy, current_node]\n",
    "        self.nodes.append(pair)\n",
    "    \n",
    "    # Creates the root node\n",
    "    def create_root_node(self):\n",
    "        entropy = self.calculate_entropy(self.dataset)\n",
    "        self.create_node(self.dataset, entropy)\n",
    "\n",
    "# Calculating with information gain\n",
    "# 1. Split the data with each condition and check which conditioned resulted in the greatest decrease in entropy after splitting\n",
    "#    (or, the greatest information gain).\n",
    "# 2. Select that condition for the first split.\n",
    "# 3. Split the remaining nodes with the remaining conditions\n",
    "\n",
    "    # Determines the optimal condition to split the previous node such that there is the greatest possible information gain.\n",
    "    # 1. Stop after 1 split.\n",
    "    # 2. Sort through an array of key-value pairs of entropy sum and feature used in the split condition.\n",
    "    # 3. The feature of the pair with the lowest entropy sum will be used in the following node split condition.\n",
    "    def optimal_condition(self, current_node):\n",
    "        for feature_index in self.feature_indicies:\n",
    "            for feature_instance in self.feature_instances[feature_index][1]:\n",
    "\n",
    "                new_node_1 = []\n",
    "                new_node_2 = []\n",
    "                \n",
    "                for instance in current_node:\n",
    "                    if instance[feature_index] == feature_instance:\n",
    "                        new_node_1.append(instance)\n",
    "                    if instance[feature_index] != feature_instance:\n",
    "                        new_node_2.append(instance)\n",
    "\n",
    "                print(f\"\\nNode 1 with a feature index of {feature_index} and a feature instance of {feature_instance}: {new_node_1}\")\n",
    "                print(f\"Node 2 with a feature index of {feature_index} and a feature instance of {feature_instance}: {new_node_2}\")\n",
    "\n",
    "                IG = self.calculate_information_gain(current_node, new_node_1, new_node_2)\n",
    "\n",
    "                triplet = [IG, feature_index, feature_instance]\n",
    "                self.possible_outcomes.append(triplet)\n",
    "                \n",
    "                self.possible_outcomes = sorted(self.possible_outcomes, reverse = True)\n",
    "                self.optimal_feature_index = self.possible_outcomes[0][1]\n",
    "                self.optimal_feature_instance = self.possible_outcomes[0][2]\n",
    "\n",
    "        print(f\"\\nAll possible outcomes of the split: {self.possible_outcomes}\")\n",
    "        print(f\"Optimal feature index: {self.optimal_feature_index}\")\n",
    "        print(f\"Optimal feature instance: {self.optimal_feature_instance}\")\n",
    "\n",
    "\n",
    "test_rf = DecisionTree(practice_training_dataset_1, 2)\n",
    "print(f\"Node: {practice_training_dataset_1}\")\n",
    "test_rf.get_features()\n",
    "test_rf.optimal_condition(test_rf.dataset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "645849c4b3fedeaec0dff6f1255f6441911479c7bed6760f3f22b01765ed8a1f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 â€” RANDOM FOREST ALGORITHM:\n",
    "# a. Split data such that information gain (decrease in entropy after splitting) is high\n",
    "# b. Split the data using each condition, check the gain returned\n",
    "# c. Condition with highest gain will be used to make the first split\n",
    "# d. Keep splitting nodes UNTIL entropy is 0\n",
    "# e. Classify unknown data point with multiple trees\n",
    "\n",
    "practice_training_dataset = [[0, 3, 0], \n",
    "                             [1, 3, 1], \n",
    "                             [2, 1, 2],  \n",
    "                             [0, 3, 0], \n",
    "                             [1, 3, 1], \n",
    "                             [2, 1, 2]]\n",
    "\n",
    "practice_unclassified_instance = [2, 1]\n",
    "\n",
    "# Entropy: Count of different classes in a list\n",
    "# Information gain: Difference between count of different classes in the current node \n",
    "#                   and the count of different classes in the previous node\n",
    "\n",
    "# Process:\n",
    "# 1. Get the entropy value of the root node\n",
    "# 2. Iterature through the database and split it into two arrays based on a condition\n",
    "# 3. Get the entropy value of the following nodes, find the difference between the \n",
    "#    root node entropy and the subsequent nodes\n",
    "# 4. Continue to split nodes with nonzero entropy until only leaf nodes remain\n",
    "\n",
    "import random\n",
    "\n",
    "# Creates a random forest class\n",
    "class RandomForest:\n",
    "    \n",
    "    # Initializes the attributes of the RandomForest class.\n",
    "    def __init__(self, dataset, number_of_features):\n",
    "        self.dataset = dataset\n",
    "        self.number_of_features = number_of_features\n",
    "        self.random_seed = 0\n",
    "\n",
    "        self.instances = []\n",
    "        self.feature_instances = []\n",
    "        self.features = []\n",
    "\n",
    "        self.feature = 0\n",
    "        self.feature_to_split = 0\n",
    "        \n",
    "        self.nodes = []\n",
    "        self.current_node = []\n",
    "\n",
    "    # Populates self.feature_instances with arrays of unique instances of each feature in the database\n",
    "    def set_random_features(self):\n",
    "        feature = 0\n",
    "        while feature < self.number_of_features:\n",
    "            for instance in self.dataset:\n",
    "                if instance[feature] not in self.instances:\n",
    "                    self.instances.append(instance[feature])\n",
    "            pair = [feature, self.instances]\n",
    "            self.feature_instances.append(pair)\n",
    "            feature += 1\n",
    "            self.instances = []\n",
    "\n",
    "        for feature_number in range(self.number_of_features):\n",
    "            self.features.append(feature_number)\n",
    "\n",
    "        self.random_seed = random.randrange(0, 101)\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "        self.feature = self.features.pop(random.choice(self.features))\n",
    "        self.feature_to_split = random.choice(self.feature_instances[self.feature][1])\n",
    "\n",
    "        # 1. Get argument: how many features there are, random seed\n",
    "        # 2. Based on the number of features, use an algo to identify the different instances of that feature\n",
    "        # 3. Every time new node is called, randomize the feature and how it is split. Once used, remove from lists.\n",
    "\n",
    "\n",
    "    # Evaluates entropy by caluclating the amount of unique classes in the node\n",
    "    # and adds node to a list of all nodes\n",
    "    def create_node(self, current_node):\n",
    "        unique_labels = []\n",
    "        for instance in current_node:\n",
    "            if instance[-1] not in unique_labels:\n",
    "                unique_labels.append(instance[-1])\n",
    "        entropy = len(unique_labels) \n",
    "        pair = [entropy, current_node]\n",
    "        self.nodes.append(pair)\n",
    "        print(self.nodes)\n",
    "    \n",
    "    # Splits previous node data into two other nodes based on a condition\n",
    "    def create_nodes(self):\n",
    "        \n",
    "        # Creates the parent node\n",
    "        self.current_node = self.dataset\n",
    "        self.create_node(self.current_node)\n",
    "\n",
    "        # Every time function is called, new nodes are created for existing nodes with nonzero entropy\n",
    "        node_counter = 0\n",
    "        entropy_count = 0\n",
    "        for node in self.nodes:\n",
    "            if node[0] != 1:\n",
    "                entropy_count += 1\n",
    "        \n",
    "            if entropy_count == 0:\n",
    "                break\n",
    "            \n",
    "            elif self.nodes[node_counter][0] != 1:\n",
    "                new_node_1 = []\n",
    "                new_node_2 = []\n",
    "                for instance in self.nodes[node_counter][1]:\n",
    "                    if instance[self.feature] == self.feature_to_split:\n",
    "                        new_node_1.append(instance)\n",
    "                    if instance[self.feature] != self.feature_to_split:\n",
    "                        new_node_2.append(instance)\n",
    "                if new_node_1:\n",
    "                    self.current_node = new_node_1\n",
    "                    self.create_node(self.current_node)\n",
    "                if new_node_2:\n",
    "                    self.current_node = new_node_2\n",
    "                    self.create_node(self.current_node)\n",
    "                node_counter += 1\n",
    "                self.set_random_features()\n",
    "            elif self.nodes[node_counter][0] == 1:\n",
    "                node_counter += 1\n",
    "                self.set_random_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, [0, 1, 2]], [1, [3, 1]]]\n",
      "0\n",
      "1\n",
      "[[3, [[0, 3, 0], [1, 3, 1], [2, 1, 2], [0, 3, 0], [1, 3, 1], [2, 1, 2]]]]\n",
      "[[3, [[0, 3, 0], [1, 3, 1], [2, 1, 2], [0, 3, 0], [1, 3, 1], [2, 1, 2]]], [1, [[1, 3, 1], [1, 3, 1]]]]\n",
      "[[3, [[0, 3, 0], [1, 3, 1], [2, 1, 2], [0, 3, 0], [1, 3, 1], [2, 1, 2]]], [1, [[1, 3, 1], [1, 3, 1]]], [2, [[0, 3, 0], [2, 1, 2], [0, 3, 0], [2, 1, 2]]]]\n",
      "[[3, [[0, 3, 0], [1, 3, 1], [2, 1, 2], [0, 3, 0], [1, 3, 1], [2, 1, 2]]], [1, [[1, 3, 1], [1, 3, 1]]], [2, [[0, 3, 0], [2, 1, 2], [0, 3, 0], [2, 1, 2]]], [1, [[0, 3, 0], [0, 3, 0]]]]\n",
      "[[3, [[0, 3, 0], [1, 3, 1], [2, 1, 2], [0, 3, 0], [1, 3, 1], [2, 1, 2]]], [1, [[1, 3, 1], [1, 3, 1]]], [2, [[0, 3, 0], [2, 1, 2], [0, 3, 0], [2, 1, 2]]], [1, [[0, 3, 0], [0, 3, 0]]], [1, [[2, 1, 2], [2, 1, 2]]]]\n",
      "[[3, [[0, 3, 0], [1, 3, 1], [2, 1, 2], [0, 3, 0], [1, 3, 1], [2, 1, 2]]], [1, [[1, 3, 1], [1, 3, 1]]], [2, [[0, 3, 0], [2, 1, 2], [0, 3, 0], [2, 1, 2]]], [1, [[0, 3, 0], [0, 3, 0]]], [1, [[2, 1, 2], [2, 1, 2]]]]\n"
     ]
    }
   ],
   "source": [
    "test_rf = RandomForest(practice_training_dataset, 2)\n",
    "test_rf.set_random_features()\n",
    "print(test_rf.feature_instances)\n",
    "print(test_rf.feature)\n",
    "print(test_rf.feature_to_split)\n",
    "test_rf.create_nodes()\n",
    "print(test_rf.nodes)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "645849c4b3fedeaec0dff6f1255f6441911479c7bed6760f3f22b01765ed8a1f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

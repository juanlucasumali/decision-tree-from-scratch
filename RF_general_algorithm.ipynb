{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Creates a decision tree\n",
    "class DecisionTree:\n",
    "    \n",
    "    # Initializes the attributes of the DecisionTree class.\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.number_of_features = 0\n",
    "        self.random_seed = 0\n",
    "\n",
    "        self.instances = []\n",
    "        self.feature_indicies = []\n",
    "        self.feature_instances = []\n",
    "        self.optimal_feature_index = 0\n",
    "        self.optimal_feature_instance = 0\n",
    "\n",
    "        self.unique_labels = []\n",
    "\n",
    "        self.feature = 0\n",
    "        self.feature_to_split = 0\n",
    "\n",
    "        self.possible_outcomes = []\n",
    "        \n",
    "        self.nodes = []\n",
    "        self.current_node = []\n",
    "\n",
    "        self.optimal_features = []\n",
    "\n",
    "\n",
    "    # Populates self.feature_instances with arrays of unique instances of each feature in the node\n",
    "    def get_features(self, node):\n",
    "        # Determining number of features within a node\n",
    "        labels = []\n",
    "        for instance in node:\n",
    "            labels.append(instance[-1])\n",
    "        \n",
    "        self.unique_labels = []\n",
    "        for instance in node:\n",
    "            if instance[-1] not in self.unique_labels:\n",
    "                self.unique_labels.append(instance[-1])\n",
    "\n",
    "        np_labels = np.array(labels)\n",
    "        for label in self.unique_labels:\n",
    "            # Using numpy to count the occurences of each label within the node\n",
    "            self.number_of_features = np.count_nonzero(np_labels == label)\n",
    "        \n",
    "        feature = 0\n",
    "        while feature < self.number_of_features:\n",
    "            for instance in self.dataset:\n",
    "                if instance[feature] not in self.instances:\n",
    "                    self.instances.append(instance[feature])\n",
    "            pair = [feature, self.instances]\n",
    "            self.feature_instances.append(pair)\n",
    "            feature += 1\n",
    "            self.instances = []\n",
    "\n",
    "        for feature_number in range(self.number_of_features):\n",
    "            self.feature_indicies.append(feature_number)\n",
    "\n",
    "        # 1. Get argument: how many features there are, random seed\n",
    "        # 2. Based on the number of features, use an algo to identify the different instances of that feature\n",
    "        # 3. Every time new node is called, randomize the feature and how it is split. Once used, remove from lists.\n",
    "\n",
    "    # Calculates the entropy of the current node\n",
    "    \n",
    "    def calculate_entropy(self, current_node):\n",
    "    \n",
    "        entropy = 0\n",
    "\n",
    "        # Create an array of the labels \n",
    "        labels = []\n",
    "        for instance in current_node:\n",
    "            labels.append(instance[-1])\n",
    "        print(f\"All classes in the dataset: {labels}\")\n",
    "        \n",
    "        unique_labels = []\n",
    "        for instance in current_node:\n",
    "            if instance[-1] not in unique_labels:\n",
    "                unique_labels.append(instance[-1])\n",
    "        print(f\"\\nAll unique labels in the dataset: {unique_labels}\")\n",
    "\n",
    "        # 1. Get the count of one unique label in labels and pop it all out (for loop with length of unique labels array)\n",
    "        # 2. Apply to the entropy formula \n",
    "        # 3. Repeat\n",
    "        \n",
    "        np_labels = np.array(labels)\n",
    "        for label in unique_labels:\n",
    "            # Using numpy to count the occurences of each label within the node\n",
    "            count = np.count_nonzero(np_labels == label)\n",
    "            entropy += -count / len(labels) * np.log(count / len(labels)) / np.log(len(unique_labels))\n",
    "        \n",
    "\n",
    "        print(f\"\\nCalculated entropy: {entropy}\")\n",
    "\n",
    "        return entropy\n",
    "\n",
    "        \n",
    "\n",
    "    def calculate_weighted_average_times_child(self, child_node_1, child_node_2):\n",
    "        \n",
    "        entropy = 0\n",
    "        weighted_average_times_children = 0\n",
    "\n",
    "        for child_node in [child_node_1, child_node_2]:\n",
    "            unique_labels = []\n",
    "            for label in child_node:\n",
    "                if label not in unique_labels:\n",
    "                    unique_labels.append(label[-1])\n",
    "            \n",
    "            np_labels = np.array(child_node)\n",
    "            for label in unique_labels:\n",
    "                count = np.count_nonzero(np_labels == label)\n",
    "                entropy += -count / len(child_node) * np.log(count / len(child_node)) / np.log(len(unique_labels))\n",
    "                weighted_average_times_children += count / len(child_node) * entropy\n",
    "        \n",
    "        return weighted_average_times_children\n",
    "    \n",
    "    def calculate_information_gain(self, current_node, child_node_1, child_node_2):\n",
    "        watc = self.calculate_weighted_average_times_child(child_node_1, child_node_2)\n",
    "        IG = self.calculate_entropy(current_node) - watc\n",
    "        return IG\n",
    "        \n",
    "    def split_root_node(self, root_node, of_index, of_instance):\n",
    "\n",
    "        child_node_1 = []\n",
    "        child_node_2 = []\n",
    "        \n",
    "        for instance in root_node:\n",
    "            if instance[of_index] == of_instance:\n",
    "                child_node_1.append(instance)\n",
    "            if instance[of_index] != of_instance:\n",
    "                child_node_2.append(instance)\n",
    "\n",
    "        print(f\"\\nOptimized Child Node 1 with a feature index of {of_index} and a feature instance of {of_instance}: {child_node_1}\")\n",
    "        print(f\"Optimized Child Node 2 with a feature index of {of_index} and a feature instance of {of_instance}: {child_node_2}\")\n",
    "\n",
    "        return child_node_1, child_node_2\n",
    "\n",
    "# Calculating with information gain\n",
    "# 1. Split the data with each condition and check which conditioned resulted in the greatest decrease in entropy after splitting\n",
    "#    (or, the greatest information gain).\n",
    "# 2. Select that condition for the first split.\n",
    "# 3. Split the remaining nodes with the remaining conditions\n",
    "\n",
    "    # Determines the optimal condition to split the previous node such that there is the greatest possible information gain.\n",
    "    # 1. Stop after 1 split.\n",
    "    # 2. Sort through an array of key-value pairs of entropy sum and feature used in the split condition.\n",
    "    # 3. The feature of the pair with the lowest entropy sum will be used in the following node split condition.\n",
    "    def optimal_condition(self, current_node):\n",
    "        for feature_index in self.feature_indicies:\n",
    "            for feature_instance in self.feature_instances[feature_index][1]:\n",
    "\n",
    "                new_node_1 = []\n",
    "                new_node_2 = []\n",
    "                \n",
    "                for instance in current_node:\n",
    "                    if instance[feature_index] == feature_instance:\n",
    "                        new_node_1.append(instance)\n",
    "                    if instance[feature_index] != feature_instance:\n",
    "                        new_node_2.append(instance)\n",
    "\n",
    "                print(f\"\\nNode 1 with a feature index of {feature_index} and a feature instance of {feature_instance}: {new_node_1}\")\n",
    "                print(f\"Node 2 with a feature index of {feature_index} and a feature instance of {feature_instance}: {new_node_2}\")\n",
    "\n",
    "                IG = self.calculate_information_gain(current_node, new_node_1, new_node_2)\n",
    "\n",
    "                triplet = [IG, feature_index, feature_instance]\n",
    "                self.possible_outcomes.append(triplet)\n",
    "                \n",
    "                self.possible_outcomes = sorted(self.possible_outcomes, reverse = True)\n",
    "                self.optimal_feature_index = self.possible_outcomes[0][1]\n",
    "                self.optimal_feature_instance = self.possible_outcomes[0][2]\n",
    "        \n",
    "        \n",
    "        print(f\"\\nAll possible outcomes of the split: {self.possible_outcomes}\")\n",
    "        print(f\"Optimal feature index: {self.optimal_feature_index}\")\n",
    "        print(f\"Optimal feature instance: {self.optimal_feature_instance}\")\n",
    "\n",
    "        pair = [self.optimal_feature_index, self.optimal_feature_instance]\n",
    "        self.optimal_features.append(pair)\n",
    "\n",
    "        print(f\"\\nOptimal features: {self.optimal_features}\")\n",
    "        \n",
    "        return self.optimal_feature_index, self.optimal_feature_instance\n",
    "    \n",
    "    def optimal_child_condition(self, child_node):\n",
    "        for feature_index in self.feature_indicies:\n",
    "            for feature_instance in self.feature_instances[feature_index][1]:\n",
    "\n",
    "                new_node_1 = []\n",
    "                new_node_2 = []\n",
    "                \n",
    "                for instance in child_node:\n",
    "                    if instance[feature_index] == feature_instance:\n",
    "                        new_node_1.append(instance)\n",
    "                    if instance[feature_index] != feature_instance:\n",
    "                        new_node_2.append(instance)\n",
    "\n",
    "                print(f\"\\nNode 1 with a feature index of {feature_index} and a feature instance of {feature_instance}: {new_node_1}\")\n",
    "                print(f\"Node 2 with a feature index of {feature_index} and a feature instance of {feature_instance}: {new_node_2}\")\n",
    "\n",
    "                IG = self.calculate_information_gain(child_node, new_node_1, new_node_2)\n",
    "\n",
    "                triplet = [IG, feature_index, feature_instance]\n",
    "                self.possible_outcomes.append(triplet)\n",
    "                \n",
    "                self.possible_outcomes = sorted(self.possible_outcomes, reverse = True)\n",
    "                self.optimal_feature_index = self.possible_outcomes[0][1]\n",
    "                self.optimal_feature_instance = self.possible_outcomes[0][2]\n",
    "        \n",
    "        \n",
    "        print(f\"\\nAll possible outcomes of the split: {self.possible_outcomes}\")\n",
    "        print(f\"Optimal feature index: {self.optimal_feature_index}\")\n",
    "        print(f\"Optimal feature instance: {self.optimal_feature_instance}\")\n",
    "\n",
    "        pair = [self.optimal_feature_index, self.optimal_feature_instance]\n",
    "        self.optimal_features.append(pair)\n",
    "\n",
    "        print(f\"\\nOptimal features: {self.optimal_features}\")\n",
    "        \n",
    "        return self.optimal_feature_index, self.optimal_feature_instance\n",
    "\n",
    "    # 0. Apply a stopping criteria\n",
    "    # 1. Find optimal features for parent mode, split data with optimal features\n",
    "    # 2. Find optimal features for the next child node, split data with optimal features\n",
    "    # 3. Repeat\n",
    "\n",
    "    def build_tree(self, root_node):\n",
    "        self.get_features(root_node)\n",
    "        of_index, of_instance = self.optimal_condition(root_node)\n",
    "        child_node_1, child_node_2 = self.split_root_node(root_node, of_index, of_instance)\n",
    "        \n",
    "        self.build_children(child_node_1, child_node_2)\n",
    "    \n",
    "    def build_children(self, child_node_1, child_node_2):\n",
    "\n",
    "        for node in [child_node_1, child_node_2]:\n",
    "            self.get_features(node)\n",
    "            if len(self.unique_labels) == 1:\n",
    "                self.optimal_features.append([-1, node[-1][-1]])\n",
    "                pass\n",
    "            else:\n",
    "                of_index, of_instance = self.optimal_child_condition(node)\n",
    "                child_node_1, child_node_2 = self.split_root_node(node, of_index, of_instance)\n",
    "                self.build_children(child_node_1, child_node_2)\n",
    "    \n",
    "    # Using the optimal features, construct a data tree that results in the unclassified instance being sorted into a leaf node.\n",
    "\n",
    "    # 1. If-else statements to traverse the data point through the trained tree.\n",
    "    # 2. If it reaches a leaf node, classify as the classes from that leaf node\n",
    "\n",
    "    # Traversing the data\n",
    "    # 1. Iterate through the optimal features from the trained dataset\n",
    "    # 2. If following value is -1, print class if passes condition\n",
    "\n",
    "    def classify_data_point(self, data_point):\n",
    "        for feature in self.optimal_features:\n",
    "            self.next_index = self.optimal_features.index(feature) + 1\n",
    "            if self.next_index == len(self.optimal_features):\n",
    "                break\n",
    "            if data_point[feature[0]] == feature[1]:\n",
    "                if self.optimal_features[self.next_index][0] == -1:\n",
    "                    print(f\"Class {self.optimal_features[self.next_index][-1]}\")\n",
    "                    break\n",
    "            else:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: [[0, 3, 0], [1, 3, 1], [2, 1, 2], [0, 3, 0], [1, 3, 1], [2, 1, 2]]\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 0: [[0, 3, 0], [0, 3, 0]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 0: [[1, 3, 1], [2, 1, 2], [1, 3, 1], [2, 1, 2]]\n",
      "All classes in the dataset: [0, 1, 2, 0, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [0, 1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 1: [[1, 3, 1], [1, 3, 1]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 1: [[0, 3, 0], [2, 1, 2], [0, 3, 0], [2, 1, 2]]\n",
      "All classes in the dataset: [0, 1, 2, 0, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [0, 1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 2: [[2, 1, 2], [2, 1, 2]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 2: [[0, 3, 0], [1, 3, 1], [0, 3, 0], [1, 3, 1]]\n",
      "All classes in the dataset: [0, 1, 2, 0, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [0, 1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 1 and a feature instance of 3: [[0, 3, 0], [1, 3, 1], [0, 3, 0], [1, 3, 1]]\n",
      "Node 2 with a feature index of 1 and a feature instance of 3: [[2, 1, 2], [2, 1, 2]]\n",
      "All classes in the dataset: [0, 1, 2, 0, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [0, 1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 1 and a feature instance of 1: [[2, 1, 2], [2, 1, 2]]\n",
      "Node 2 with a feature index of 1 and a feature instance of 1: [[0, 3, 0], [1, 3, 1], [0, 3, 0], [1, 3, 1]]\n",
      "All classes in the dataset: [0, 1, 2, 0, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [0, 1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "All possible outcomes of the split: [[36.29041406655651, 0, 0], [29.0, 1, 1], [29.0, 0, 2], [29.0, 0, 1], [13.0, 1, 3]]\n",
      "Optimal feature index: 0\n",
      "Optimal feature instance: 0\n",
      "\n",
      "Optimal features: [[0, 0]]\n",
      "\n",
      "Optimized Child Node 1 with a feature index of 0 and a feature instance of 0: [[0, 3, 0], [0, 3, 0]]\n",
      "Optimized Child Node 2 with a feature index of 0 and a feature instance of 0: [[1, 3, 1], [2, 1, 2], [1, 3, 1], [2, 1, 2]]\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 0: []\n",
      "Node 2 with a feature index of 0 and a feature instance of 0: [[1, 3, 1], [2, 1, 2], [1, 3, 1], [2, 1, 2]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 1: [[1, 3, 1], [1, 3, 1]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 1: [[2, 1, 2], [2, 1, 2]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 2: [[2, 1, 2], [2, 1, 2]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 2: [[1, 3, 1], [1, 3, 1]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 1 and a feature instance of 3: [[1, 3, 1], [1, 3, 1]]\n",
      "Node 2 with a feature index of 1 and a feature instance of 3: [[2, 1, 2], [2, 1, 2]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 1 and a feature instance of 1: [[2, 1, 2], [2, 1, 2]]\n",
      "Node 2 with a feature index of 1 and a feature instance of 1: [[1, 3, 1], [1, 3, 1]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 0: []\n",
      "Node 2 with a feature index of 0 and a feature instance of 0: [[1, 3, 1], [2, 1, 2], [1, 3, 1], [2, 1, 2]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 1: [[1, 3, 1], [1, 3, 1]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 1: [[2, 1, 2], [2, 1, 2]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 2: [[2, 1, 2], [2, 1, 2]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 2: [[1, 3, 1], [1, 3, 1]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 1 and a feature instance of 3: [[1, 3, 1], [1, 3, 1]]\n",
      "Node 2 with a feature index of 1 and a feature instance of 3: [[2, 1, 2], [2, 1, 2]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 1 and a feature instance of 1: [[2, 1, 2], [2, 1, 2]]\n",
      "Node 2 with a feature index of 1 and a feature instance of 1: [[1, 3, 1], [1, 3, 1]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 0: []\n",
      "Node 2 with a feature index of 0 and a feature instance of 0: [[1, 3, 1], [2, 1, 2], [1, 3, 1], [2, 1, 2]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 1: [[1, 3, 1], [1, 3, 1]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 1: [[2, 1, 2], [2, 1, 2]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 0 and a feature instance of 2: [[2, 1, 2], [2, 1, 2]]\n",
      "Node 2 with a feature index of 0 and a feature instance of 2: [[1, 3, 1], [1, 3, 1]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 1 and a feature instance of 3: [[1, 3, 1], [1, 3, 1]]\n",
      "Node 2 with a feature index of 1 and a feature instance of 3: [[2, 1, 2], [2, 1, 2]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "Node 1 with a feature index of 1 and a feature instance of 1: [[2, 1, 2], [2, 1, 2]]\n",
      "Node 2 with a feature index of 1 and a feature instance of 1: [[1, 3, 1], [1, 3, 1]]\n",
      "All classes in the dataset: [1, 2, 1, 2]\n",
      "\n",
      "All unique labels in the dataset: [1, 2]\n",
      "\n",
      "Calculated entropy: 1.0\n",
      "\n",
      "All possible outcomes of the split: [[41.0, 1, 3], [41.0, 1, 3], [41.0, 1, 3], [41.0, 1, 1], [41.0, 1, 1], [41.0, 1, 1], [41.0, 0, 2], [41.0, 0, 2], [41.0, 0, 2], [41.0, 0, 1], [41.0, 0, 1], [41.0, 0, 1], [36.29041406655651, 0, 0], [29.0, 1, 1], [29.0, 0, 2], [29.0, 0, 1], [13.0, 1, 3], [4.290414066556504, 0, 0], [4.290414066556504, 0, 0], [4.290414066556504, 0, 0]]\n",
      "Optimal feature index: 1\n",
      "Optimal feature instance: 3\n",
      "\n",
      "Optimal features: [[0, 0], [-1, 0], [1, 3]]\n",
      "\n",
      "Optimized Child Node 1 with a feature index of 1 and a feature instance of 3: [[1, 3, 1], [1, 3, 1]]\n",
      "Optimized Child Node 2 with a feature index of 1 and a feature instance of 3: [[2, 1, 2], [2, 1, 2]]\n",
      "\n",
      "Optimal features: [[0, 0], [-1, 0], [1, 3], [-1, 1], [-1, 2]]\n",
      "Class 0\n",
      "Class 1\n",
      "Class 2\n",
      "Class 0\n",
      "Class 1\n",
      "Class 2\n"
     ]
    }
   ],
   "source": [
    "practice_training_dataset_1 = [[0, 3, 0], \n",
    "                             [1, 3, 1], \n",
    "                             [2, 1, 2],  \n",
    "                             [0, 3, 0], \n",
    "                             [1, 3, 1], \n",
    "                             [2, 1, 2]]\n",
    "\n",
    "# Training the model\n",
    "train_rf = DecisionTree(practice_training_dataset_1)\n",
    "print(f\"Node: {practice_training_dataset_1}\")\n",
    "train_rf.build_tree(practice_training_dataset_1)\n",
    "print(f\"\\nOptimal features: {train_rf.optimal_features}\")\n",
    "\n",
    "# Verifying the model's accuracy\n",
    "practice_unclassified_instance = [0, 3]\n",
    "train_rf.classify_data_point(practice_unclassified_instance)\n",
    "\n",
    "practice_unclassified_instance = [1, 3]\n",
    "train_rf.classify_data_point(practice_unclassified_instance)\n",
    "\n",
    "practice_unclassified_instance = [2, 1]\n",
    "train_rf.classify_data_point(practice_unclassified_instance)\n",
    "\n",
    "practice_unclassified_instance = [0, 3]\n",
    "train_rf.classify_data_point(practice_unclassified_instance)\n",
    "\n",
    "practice_unclassified_instance = [1, 3]\n",
    "train_rf.classify_data_point(practice_unclassified_instance)\n",
    "\n",
    "practice_unclassified_instance = [2, 1]\n",
    "train_rf.classify_data_point(practice_unclassified_instance)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "645849c4b3fedeaec0dff6f1255f6441911479c7bed6760f3f22b01765ed8a1f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: [[0, 3, 0], [1, 3, 1], [2, 1, 2], [0, 3, 0], [1, 3, 1], [2, 1, 2]]\n",
      "Split node by this feature index: 0\n",
      "Split node by this feature: 1\n",
      "[[1, 3, 1], [1, 3, 1]]\n",
      "[[0, 3, 0], [2, 1, 2], [0, 3, 0], [2, 1, 2]]\n",
      "Outcome of the split [total entropy, feature used in the condition]: [[3, 0]]\n"
     ]
    }
   ],
   "source": [
    "# STEP 1 â€” RANDOM FOREST ALGORITHM:\n",
    "# a. Split data such that information gain (decrease in entropy after splitting) is high\n",
    "# b. Split the data using each condition, check the gain returned\n",
    "# c. Condition with highest gain will be used to make the first split\n",
    "# d. Keep splitting nodes UNTIL entropy is 0\n",
    "# e. Classify unknown data point with multiple trees\n",
    "\n",
    "practice_training_dataset = [[0, 3, 0], \n",
    "                             [1, 3, 1], \n",
    "                             [2, 1, 2],  \n",
    "                             [0, 3, 0], \n",
    "                             [1, 3, 1], \n",
    "                             [2, 1, 2]]\n",
    "\n",
    "practice_unclassified_instance = [2, 1]\n",
    "\n",
    "# Entropy: Count of different classes in a list\n",
    "# Information gain: Difference between count of different classes in the current node \n",
    "#                   and the count of different classes in the previous node\n",
    "\n",
    "# Process:\n",
    "# 1. Get the entropy value of the root node\n",
    "# 2. Iterature through the database and split it into two arrays based on a condition\n",
    "# 3. Get the entropy value of the following nodes, find the difference between the \n",
    "#    root node entropy and the subsequent nodes\n",
    "# 4. Continue to split nodes with nonzero entropy until only leaf nodes remain\n",
    "\n",
    "import random\n",
    "\n",
    "# Creates a random forest class\n",
    "class RandomForest:\n",
    "    \n",
    "    # Initializes the attributes of the RandomForest class.\n",
    "    def __init__(self, dataset, number_of_features):\n",
    "        self.dataset = dataset\n",
    "        self.number_of_features = number_of_features\n",
    "        self.random_seed = 0\n",
    "\n",
    "        self.instances = []\n",
    "        self.feature_instances = []\n",
    "        self.features = []\n",
    "\n",
    "        self.feature = 0\n",
    "        self.feature_to_split = 0\n",
    "\n",
    "        self.possible_outcomes = []\n",
    "        \n",
    "        self.nodes = []\n",
    "        self.current_node = []\n",
    "\n",
    "    # Populates self.feature_instances with arrays of unique instances of each feature in the database\n",
    "    def set_random_features(self):\n",
    "        feature = 0\n",
    "        while feature < self.number_of_features:\n",
    "            for instance in self.dataset:\n",
    "                if instance[feature] not in self.instances:\n",
    "                    self.instances.append(instance[feature])\n",
    "            pair = [feature, self.instances]\n",
    "            self.feature_instances.append(pair)\n",
    "            feature += 1\n",
    "            self.instances = []\n",
    "\n",
    "        for feature_number in range(self.number_of_features):\n",
    "            self.features.append(feature_number)\n",
    "\n",
    "        self.random_seed = random.randrange(0, 101)\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "        self.feature = self.features.pop(random.choice(self.features))\n",
    "        self.feature_to_split = random.choice(self.feature_instances[self.feature][1])\n",
    "\n",
    "        # 1. Get argument: how many features there are, random seed\n",
    "        # 2. Based on the number of features, use an algo to identify the different instances of that feature\n",
    "        # 3. Every time new node is called, randomize the feature and how it is split. Once used, remove from lists.\n",
    "\n",
    "    # Calculates the entropy of the current node\n",
    "    def calculate_entropy(self, current_node):\n",
    "        unique_labels = []\n",
    "        for instance in current_node:\n",
    "            if instance[-1] not in unique_labels:\n",
    "                unique_labels.append(instance[-1])\n",
    "        entropy = len(unique_labels) \n",
    "        return entropy\n",
    "        \n",
    "    # Adds the current node to a list of all nodes as a key-value pair with entropy as its key\n",
    "    def create_node(self, current_node, entropy):\n",
    "        pair = [entropy, current_node]\n",
    "        self.nodes.append(pair)\n",
    "    \n",
    "    # Creates the root node\n",
    "    def create_root_node(self):\n",
    "        entropy = self.calculate_entropy(self.dataset)\n",
    "        self.create_node(self.dataset, entropy)\n",
    "\n",
    "# Calculating with information gain\n",
    "# 1. Split the data with each condition and check which conditioned resulted in the greatest decrease in entropy after splitting\n",
    "#    (or, the greatest information gain).\n",
    "# 2. Select that condition for the first split.\n",
    "# 3. Split the remaining nodes with the remaining conditions\n",
    "\n",
    "    # Determines the optimal condition to split the previous node such that there is the greatest possible information gain.\n",
    "    # 1. Stop after 1 split.\n",
    "    # 2. Sort through an array of key-value pairs of entropy sum and feature used in the split condition.\n",
    "    # 3. The feature of the pair with the lowest entropy sum will be used in the following node split condition.\n",
    "    def optimal_condition(self, current_node):\n",
    "        new_node_1 = []\n",
    "        new_node_2 = []\n",
    "        entropy_sum = 0\n",
    "        \n",
    "        for instance in current_node:\n",
    "            if instance[self.feature] == self.feature_to_split:\n",
    "                new_node_1.append(instance)\n",
    "            if instance[self.feature] != self.feature_to_split:\n",
    "                new_node_2.append(instance)\n",
    "        print(new_node_1)\n",
    "        print(new_node_2)\n",
    "\n",
    "        if new_node_1:\n",
    "            entropy = self.calculate_entropy(new_node_1)\n",
    "            entropy_sum += entropy\n",
    "        if new_node_2:\n",
    "            entropy = self.calculate_entropy(new_node_2)\n",
    "            entropy_sum += entropy\n",
    "\n",
    "        pair = [entropy_sum, self.feature]\n",
    "        self.possible_outcomes.append(pair)\n",
    "\n",
    "        self.set_random_features()\n",
    "\n",
    "test_rf = RandomForest(practice_training_dataset, 2)\n",
    "print(f\"Node: {practice_training_dataset}\")\n",
    "test_rf.set_random_features()\n",
    "print(f\"Split node by this feature index: {test_rf.feature}\")\n",
    "print((f\"Split node by this feature: {test_rf.feature_to_split}\"))\n",
    "test_rf.optimal_condition(test_rf.dataset)\n",
    "print((f\"Outcome of the split [total entropy, feature used in the condition]: {test_rf.possible_outcomes}\"))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "645849c4b3fedeaec0dff6f1255f6441911479c7bed6760f3f22b01765ed8a1f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
